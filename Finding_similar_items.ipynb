{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction- Finding Similar Items:\n",
    "\n",
    "    This notebook’s objective is to put the steps of textually related document dis\u0002covery based on Jaccard similarity into practice. The methods used to address the issue in a more effective manner are shingling, minhashing, and Locality Sensitive Hashing (LSH). The methods are used to examine documents downloaded from ArXiv.org \n",
    "\n",
    "### Techniques:\n",
    "\n",
    "    1. Shingling - The k-shingles, or individual substrings of length k, that make up the document can be extracted using the shingling approach. Look at the generate shingles method. Once the shingles are made, it computes a hash value for each unique shingle, and represents the document in the form of an ordered set of its hashed k-shingles. Shingle size we gave was 10. Then we compare 2 sets of shingles using Jaccard similarity.\n",
    "\n",
    "\n",
    "    2. MinHashing: The set of shingles has the disadvantage of being difficult to store due to their size. To estimate the Jaccard similarity of the un\u0002derlying sets, one method is to create signatures that can be compared between documents. Verify the min hashing method. Signatures were generated with the help of (a*x + b)/c. a,b and c are random coefficients and x is the shingles we generated previously. For each hash function, all the shingles are hashed and then the minimum one is selected. We calculate the similarity between minhashed signatures.\n",
    "\n",
    "\n",
    "    3. LSH: Local Sensitive Hashing: LSH aims to generate, from the set of signatures, a reduced list of candidate pairs, which would be the ones which similarity must be evaluated. Check LSH method. We need under\u0002standing of DHT for this. Similar items end up in the same buckets (rows).\n",
    "\n",
    "\n",
    "    4. Pre-processing of data: Once we download all the files, we also need to pre process the data. we convert text to be in lowercase, with only alphanumeric characters and single spaces. This way we optimize it to have a more accurate similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxO_Xr10H0qq"
   },
   "source": [
    "### Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASA5MgayqE5s",
    "outputId": "c8f89b57-7531-43ae-ac04-2ca3a5fbb1c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in /Users/deepakshankar/opt/anaconda3/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: feedparser in /Users/deepakshankar/opt/anaconda3/lib/python3.9/site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in /Users/deepakshankar/opt/anaconda3/lib/python3.9/site-packages (from feedparser->arxiv) (1.0.0)\n",
      "Requirement already satisfied: pdfminer.six in /Users/deepakshankar/opt/anaconda3/lib/python3.9/site-packages (20221105)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/deepakshankar/opt/anaconda3/lib/python3.9/site-packages (from pdfminer.six) (37.0.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/deepakshankar/opt/anaconda3/lib/python3.9/site-packages (from pdfminer.six) (2.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/deepakshankar/opt/anaconda3/lib/python3.9/site-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/deepakshankar/opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n"
     ]
    }
   ],
   "source": [
    "# Run only once to install necessary library\n",
    "!pip install arxiv\n",
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "wxGqH9h4H2kA"
   },
   "outputs": [],
   "source": [
    "#import the necessary modules\n",
    "import arxiv\n",
    "from pdfminer.high_level import extract_text\n",
    "from os import listdir\n",
    "import re # Subsitute in string\n",
    "import collections # Count collisions\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "from itertools import combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIqX2CKIGv-2"
   },
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7pEtVuMGyO6"
   },
   "source": [
    "### Fetch documents\n",
    "Fetch **number_of_documents** documents within the topic **topic** and stores them to the directory **dir**, previously reseting the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "QaPpFz1KJtGb"
   },
   "outputs": [],
   "source": [
    "def fetch_documents(number_of_documents, topic, dirpath):\n",
    "    # Create folder corpus where all documents will be saved \n",
    "    !rm -rf $dirpath\n",
    "    !mkdir $dirpath\n",
    "    \n",
    "    # Query papers containing quantum and save to corupus folder\n",
    "    t_result = arxiv.Search(query=topic, max_results=number_of_documents, sort_by = arxiv.SortCriterion.SubmittedDate)\n",
    "    \n",
    "    # Download pdfs and store in dirpath, returning the full path\n",
    "    document_paths = [paper.download_pdf(dirpath) for paper in t_result.results()]\n",
    "    print(\"Document Paths:\" + str(document_paths))\n",
    "    return document_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BTOaFgjqaSZ"
   },
   "source": [
    "### Extract text from documents\n",
    "Convert pdfs to text using the library pdfminer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "PJyTh0s-qvAG"
   },
   "outputs": [],
   "source": [
    "def extract_texts_from_documents(dirpath):\n",
    "    texts = [extract_text(dirpath + document_path) for document_path in listdir(dirpath)]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "bsYkOE0T-Aeg"
   },
   "outputs": [],
   "source": [
    "def extract_text_from_document(document_path):\n",
    "    return extract_text(document_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWaiMwyCwAiH"
   },
   "source": [
    "### Preprocess text in the docs\n",
    "It converts text to be in lowercase, with only alphanumeric characters and single spaces. This way we optimize it to have a more accurate similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "81c-GvbNwQJl"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(raw_text):\n",
    "    # Convert the text to lowercase (to have a more accurate similarity)\n",
    "    text = raw_text.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Replace multiple spaces with one space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvsZ9xDvuCFY"
   },
   "source": [
    "### Generate shingles\n",
    "Constructs k–shingles of a given length **k** from a given document, computes a hash value for each unique shingle, and represents the document in the form of an ordered set of its hashed k-shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "9fi6XMl7uRI0"
   },
   "outputs": [],
   "source": [
    "def generate_shingles(text, k):\n",
    "    shingles_raw = [text[i:i + k] for i in range(len(text) - k + 1)]\n",
    "    shingles = np.array(shingles_raw)\n",
    "    \n",
    "    # Remove duplicates. Do not use set or we will be loosing the order which is very important.\n",
    "    _, unique_indexes = np.unique(shingles, return_index=True)\n",
    "    return shingles[np.sort(unique_indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AFIiw_sC2WUo",
    "outputId": "ab3a8e2e-4c14-430a-8c97-be9e9343fba1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Finding ', 'inding S', 'nding Si', 'ding Sim', 'ing Simi',\n",
       "       'ng Simil', 'g Simila', ' Similar', 'Similar ', 'imilar I',\n",
       "       'milar It', 'ilar Ite', 'lar Item', 'ar Items', 'r Items ',\n",
       "       ' Items A', 'Items As', 'tems Ass', 'ems Assi', 'ms Assig',\n",
       "       's Assign', ' Assignm', 'Assignme', 'ssignmen', 'signment'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the shingles method. Expected result for string \"abcdabd\" k = 2 => {ab, bc, cd, da, bd}\n",
    "generate_shingles(\"Finding Similar Items Assignment\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPDNSIh-56Ir"
   },
   "source": [
    "Compress shingles of length k to integers, using a max of 4B per shingle (python integer internal size). [More Info](https://cp-algorithms.com/string/string-hashing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "BDZ0FiCC51tJ"
   },
   "outputs": [],
   "source": [
    "def compress_shingles(shingles):\n",
    "    hashed_shingles_raw = [hash(shingle) for shingle in shingles]\n",
    "    hashed_shingles = np.array(hashed_shingles_raw)\n",
    "    # Calculate number of collissions (before making them unique)\n",
    "    # collisions = [item for item, count in collections.Counter(shingles_hashes).items() if count > 1]\n",
    "    # print(\"Number of collisions in \" + str(len(shingles_hashes)) + \" hashes: \" + str(len(collisions)))\n",
    "\n",
    "    # Make them unique (in case there was any collition)\n",
    "    _, unique_indexes = np.unique(hashed_shingles, return_index=True)\n",
    "    return hashed_shingles[np.sort(unique_indexes.astype(int))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fudPbYY7oNW",
    "outputId": "e862132e-4107-49e7-d55b-0bbabd8bb80f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529978468396178906\n",
      "6368961987904669937\n"
     ]
    }
   ],
   "source": [
    "# Check python hash does not change in same execution (same seed)\n",
    "print(hash(\"Python Program\"))\n",
    "print(hash(\"Python Programming\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zp-9ykBOmsl6",
    "outputId": "24c83744-b528-42b9-ba18-7cedd5d694d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8835640773928410993,  4726876478362840613,   -79410858703923515,\n",
       "        6200514738019426673,  3583287604262703370, -6046518032616318703,\n",
       "        2056154539791797424, -7716825307450758219, -3238142230457423145,\n",
       "         995037586099824844, -5457119415567397071,  6964624170836775465,\n",
       "       -7085547166138316980,  5771509237749396095, -4141795931189253991,\n",
       "       -5689591536957201536, -6486996600658756250, -7081848034097264030,\n",
       "       -3834493178805496687,  8081179211975691000,  2313303806795681158,\n",
       "       -8100355998069846127, -6828396107182270118,  9182160988969257193,\n",
       "        4527192695875843020])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try shingles method.\n",
    "compress_shingles(generate_shingles(\"Finding Similar Items Assignment\", 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7ShhuOuscAW",
    "outputId": "ddec3cfb-5002-4d47-81d6-ff9c6151401a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8835640773928410993,\n",
       " 4726876478362840613,\n",
       " -79410858703923515,\n",
       " 6200514738019426673,\n",
       " 3583287604262703370,\n",
       " -6046518032616318703,\n",
       " 2056154539791797424,\n",
       " -7716825307450758219,\n",
       " -3238142230457423145,\n",
       " 995037586099824844,\n",
       " -5457119415567397071,\n",
       " 6964624170836775465,\n",
       " -7085547166138316980,\n",
       " 5771509237749396095,\n",
       " -4141795931189253991,\n",
       " -5689591536957201536,\n",
       " -6486996600658756250,\n",
       " -7081848034097264030,\n",
       " -3834493178805496687,\n",
       " 8081179211975691000,\n",
       " 2313303806795681158,\n",
       " -8100355998069846127,\n",
       " -6828396107182270118,\n",
       " 9182160988969257193,\n",
       " 4527192695875843020]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try shingles method with system provided hashing.\n",
    "[hash(shingle) for shingle in generate_shingles(\"Finding Similar Items Assignment\", 8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rAjjstjCd85"
   },
   "source": [
    "### Compare two sets of shingles using Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "9SzZSe1lCjXX"
   },
   "outputs": [],
   "source": [
    "def get_jaccard_similarity_from_shingles(shingles_1, shingles_2):\n",
    "    # Here we can use set because we are not taking into account order for the intersection\n",
    "    intersection_set = np.intersect1d(shingles_1, shingles_2)\n",
    "    \n",
    "    intersection = len(intersection_set)\n",
    "    union = len(shingles_1) + len(shingles_2) - intersection\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPWnd_k05e6S"
   },
   "source": [
    "### Random coefficients generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZQWCEIR5UwD"
   },
   "source": [
    "Make lists of random numbers with length equal to the signature, to be used in generating random hashing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "AjI9HcnwgyIO"
   },
   "outputs": [],
   "source": [
    "max_shingle_hash = sys.maxsize # defines the range for random coefficients (could be changed)\n",
    "def random_coefficients(signature_length):\n",
    "    random_list = np.array([], dtype=np.uint8)\n",
    "    for i in range(signature_length):\n",
    "        random_index = random.randint(1, max_shingle_hash)\n",
    "        while random_index in random_list: # Ensure uniqueness\n",
    "            random_index = random.randint(1, max_shingle_hash)\n",
    "        random_list = np.append(random_list, random_index)\n",
    "    \n",
    "    return random_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsVyI8ETivgk"
   },
   "source": [
    "### Generate minhash signature from shingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMQZ8gR-5lpb"
   },
   "source": [
    "For each hash function, all the shingles are hashed and then the minimum one is chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "nSszj8HupdYT"
   },
   "outputs": [],
   "source": [
    "def minhashing(signature_length, shingles, a_hash_coefficients, b_hash_coefficients, max_hash):\n",
    "    signature = np.array([], dtype=np.uint8)\n",
    "    \n",
    "    for i in range(0, signature_length):\n",
    "        hashes = ((a_hash_coefficients[i] * shingles) + b_hash_coefficients[i]) % max_hash\n",
    "        signature = np.append(signature, np.amin(hashes))\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tVIhz0q5rEz"
   },
   "source": [
    "### Similarity between minhash signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eo99Xvif5tTj"
   },
   "source": [
    "Calculate proportion of similar elements (minhashes) from two signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "aErIjsuI2-4N"
   },
   "outputs": [],
   "source": [
    "def get_similarity_minhash_signatures(signature_1, signature_2):\n",
    "    count = 0\n",
    "    for i in range(0, len(signature_1)):\n",
    "        if str(signature_1[i]) == str(signature_2[i]):\n",
    "            count += 1\n",
    "    return count / len(signature_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLXo_5IPldBQ"
   },
   "source": [
    "### LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvXSDzsk53fW"
   },
   "source": [
    "Finding best tradeoff between the bands and the rows in order to obtain the similarity closest to the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "cxaYaZoxzOLW"
   },
   "outputs": [],
   "source": [
    "def calculate_lsh_params(signature_length, threshold):\n",
    "  min = 1\n",
    "  result = (1, 1)\n",
    "\n",
    "  for i in range(1, signature_length + 1):\n",
    "    band_number = i\n",
    "    rows_number = signature_length / i\n",
    "\n",
    "    local_min = abs(threshold - (1 / band_number) ** (1 / rows_number))\n",
    "    if min > local_min:\n",
    "      min = local_min\n",
    "      result = (band_number, rows_number)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCmxC4tI6BGq"
   },
   "source": [
    "Main LSH method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "4XklnynJmNDy"
   },
   "outputs": [],
   "source": [
    "# Result => Pairs of candidates\n",
    "def lsh(signatures, signature_length, threshold, lsh_buckets):\n",
    "  candidates_pairs = set()\n",
    "  best_params = calculate_lsh_params(signature_length, threshold)\n",
    "\n",
    "  lsh_bands = best_params[0]\n",
    "  rows_number = int(best_params[1])\n",
    "\n",
    "  # Calculate coefficients of hash function\n",
    "  a_hash_coefficients = random_coefficients(lsh_bands)\n",
    "  b_hash_coefficients = random_coefficients(lsh_bands)  \n",
    "\n",
    "  for band_index in range(lsh_bands):\n",
    "    band_row_start = band_index * rows_number\n",
    "    band_signature_hashes = {} # []\n",
    "\n",
    "    # Calculate subsignature reduction or intermediate hash for each subsignature\n",
    "    subsignatures_reductions_raw = [hash(str(s[band_row_start:band_row_start + rows_number])) for s in signatures]\n",
    "    subsignatures_reductions = np.array(subsignatures_reductions_raw)\n",
    "    subsignature_hashes = (a_hash_coefficients[band_index] * subsignatures_reductions + b_hash_coefficients[band_index]) % lsh_buckets\n",
    "\n",
    "    # For each subsignature in the band, calculate its hash\n",
    "    for subsignature_hash_index in range(len(subsignature_hashes)):\n",
    "      subsignature_hash = subsignature_hashes[subsignature_hash_index]\n",
    "      if subsignature_hash not in band_signature_hashes:\n",
    "        band_signature_hashes[subsignature_hash] = np.array([], dtype=np.uint8)\n",
    "      band_signature_hashes[subsignature_hash] = np.append(band_signature_hashes[subsignature_hash], [subsignature_hash_index])\n",
    "\n",
    "    # Calculate signatures index with same hash (instead of saving it in memory) (optimization ignored)\n",
    "    # for i in range(len(band_signature_hashes) - 1):\n",
    "    #   for j in range(i + 1, len(band_signature_hashes)):\n",
    "    #     if band_signature_hashes[i] == band_signature_hashes[j]:\n",
    "    #       candidates_pairs.add((i, j))\n",
    "\n",
    "    for (key, signature_indexes) in band_signature_hashes.items():\n",
    "      if len(signature_indexes) >= 2:\n",
    "        for candidates_pair in list(combinations(signature_indexes, 2)):\n",
    "          candidates_pairs.add(candidates_pair)\n",
    "\n",
    "  return candidates_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GknrngfQ9SA_"
   },
   "source": [
    "## Wrapper methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOTpEGjp9UG7"
   },
   "source": [
    "Get compressed shingles from raw document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "U-RyNG6R9ZFc"
   },
   "outputs": [],
   "source": [
    "def get_shingles_from_document(document_path, shingle_size):\n",
    "  raw_text = extract_text_from_document(document_path)\n",
    "  text = preprocess_text(raw_text)\n",
    "  shingles = generate_shingles(text, shingle_size)\n",
    "  shingles_hashes = compress_shingles(shingles)\n",
    "  return shingles_hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZg5g1Lu-VVi"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DhniTxDv-qzk",
    "outputId": "f835d738-c0d0-4ec3-98ab-a15d58cd047d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Paths:['./corpus/2211.05783v1.Unifying_Flow_Stereo_and_Depth_Estimation.pdf', './corpus/2211.05781v1.Demystify_Transformers_Convolutions_in_Modern_Image_Deep_Networks.pdf']\n",
      "['./corpus/2211.05783v1.Unifying_Flow_Stereo_and_Depth_Estimation.pdf', './corpus/2211.05781v1.Demystify_Transformers_Convolutions_in_Modern_Image_Deep_Networks.pdf']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./corpus/2211.05783v1.Unifying_Flow_Stereo_and_Depth_Estimation.pdf'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_folder = \"./corpus/\"\n",
    "corpus_topic = \"computing\"\n",
    "corpus_size = 2\n",
    "shingles_size = 10\n",
    "\n",
    "documents_paths = fetch_documents(corpus_size, corpus_topic, corpus_folder)\n",
    "print(documents_paths)\n",
    "document_path = documents_paths[0]\n",
    "document_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yFP-GKhBocs4",
    "outputId": "e35aca6a-8275-49c4-a8f9-8ef1751278ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2\\n2\\n0\\n2\\n\\nv\\no\\nN\\n0\\n1\\n\\n]\\n\\nV\\nC\\n.\\ns\\nc\\n[\\n\\n1\\nv\\n3\\n8\\n7\\n5\\n0\\n.\\n1\\n1\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\n1\\n\\nUnifying Flow, Stereo an'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get text from document\n",
    "raw_text = extract_text_from_document(document_path)\n",
    "raw_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "1IHfw3SxxaJp",
    "outputId": "f367ba3e-d166-445e-c627-6e49ff838505"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 2 0 2 v o n 0 1 v c s c 1 v 3 8 7 5 0 1 1 2 2 v i x r a 1 unifying flow stereo and depth estimatio'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess text \n",
    "text = preprocess_text(raw_text)\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxq7AO1-xHxR",
    "outputId": "15a44c4a-7b7e-4c96-e740-d5e69921be2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' and depth', 'and depth ', 'nd depth e', 'd depth es',\n",
       "       ' depth est', 'depth esti', 'epth estim', 'pth estima',\n",
       "       'th estimat', 'h estimati', ' estimatio', 'estimation',\n",
       "       'stimation ', 'timation h', 'imation ha', 'mation hao',\n",
       "       'ation haof', 'tion haofe', 'ion haofei', 'on haofei '],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get shingles from text\n",
    "shingles = generate_shingles(text, shingles_size)\n",
    "shingles[80:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0Fv2Ixd6MVD",
    "outputId": "91eb8d4e-e32e-4916-d494-7b54ee0e7808"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8658003053516003321, -3994829778254793040, -6315432045472011917,\n",
       "        4151595403045524637, -1575478041261698680, -8805599291236292472,\n",
       "       -8643194565403864947, -7226473986899113720, -7059355869896878031,\n",
       "        -332474033304052463, -3076810432844058040,  3791098296494132965,\n",
       "        2806947349099816518, -3600403000144641366, -4922810500188328970,\n",
       "        3731345963439082979, -8817813039902184920,  9138067246647337994,\n",
       "        8419244025546067236,  8888199611919449826])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get compressed shingles\n",
    "shingles_hashes = compress_shingles(shingles)\n",
    "shingles_hashes[80:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JMKvCiRpVlg"
   },
   "source": [
    "## Main method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47qOgvpk-1mH"
   },
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "52Ldz2AsrpEe"
   },
   "outputs": [],
   "source": [
    "corpus_folder = \"./corpus/\"\n",
    "corpus_topic = \"computing\"\n",
    "corpus_size = 20\n",
    "shingle_size = 10\n",
    "\n",
    "# Minhash\n",
    "max_hash_minhash = 4294967311 # big prime number that can be used for hashing\n",
    "signature_length = 1000\n",
    "\n",
    "# LSH Config\n",
    "lsh_custom_threshold = 0.3 # It should be multiple of signature length\n",
    "lsh_buckets = max_hash_minhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bs4UPPa6EXw",
    "outputId": "da50032c-8a0c-409a-81fd-b925c29f3252",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223, 4.484304932735426)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print params => (band, rows_number)\n",
    "calculate_lsh_params(signature_length, lsh_custom_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qbd9SlmhroaG",
    "outputId": "b29d0fd6-effc-49dd-c8e6-cb8d2cc71a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Paths:['./corpus/2211.05783v1.Unifying_Flow_Stereo_and_Depth_Estimation.pdf', './corpus/2211.05781v1.Demystify_Transformers_Convolutions_in_Modern_Image_Deep_Networks.pdf', './corpus/2211.05778v1.InternImage_Exploring_Large_Scale_Vision_Foundation_Models_with_Deformable_Convolutions.pdf', './corpus/2211.05776v1.Fine_Grained_Entity_Segmentation.pdf', './corpus/2211.05774v1.NLO_computation_of_diffractive_di_hadron_production_in_a_saturation_framework.pdf', './corpus/2211.05773v1.Scaling_Neural_Face_Synthesis_to_High_FPS_and_Low_Latency_by_Neural_Caching.pdf', './corpus/2211.05770v1.StyleNAT_Giving_Each_Head_a_New_Perspective.pdf', './corpus/2211.05762v1.Efficient_brain_age_prediction_from_3D_MRI_volumes_using_2D_projections.pdf', './corpus/2211.05758v1.Cloaking_a_qubit_in_a_cavity.pdf', './corpus/2211.05756v1.Massively_Multilingual_ASR_on_70_Languages_Tokenization_Architecture_and_Generalization_Capabilities.pdf', './corpus/2211.05750v1.Nano_Nested_Human_in_the_Loop_Reward_Learning_for_Few_shot_Language_Model_Control.pdf', './corpus/2211.05749v1.Sketched_Gaussian_Model_Linear_Discriminant_Analysis_via_the_Randomized_Kaczmarz_Method.pdf', './corpus/2211.05739v1.FedLesScan_Mitigating_Stragglers_in_Serverless_Federated_Learning.pdf', './corpus/2211.05737v1.Canonical_Subproblems_for_Robot_Inverse_Kinematics.pdf', './corpus/2211.05733v1.RAPIDx_High_performance_ReRAM_Processing_in_Memory_Accelerator_for_Sequence_Alignment.pdf', './corpus/2211.05732v1.The_Sample_Complexity_of_Online_Contract_Design.pdf', './corpus/2211.05730v1.NEON_Enabling_Efficient_Support_for_Nonlinear_Operations_in_Resistive_RAM_based_Neural_Network_Accelerators.pdf', './corpus/2211.05728v1.Quantum_Power_Flows_From_Theory_to_Practice.pdf', './corpus/2211.05725v1.Quantum_key_distribution_rates_from_semidefinite_programming.pdf', './corpus/2211.05722v1.Beam_functions_for_N_jettiness_at_N_3_LO_in_perturbative_QCD.pdf']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./corpus/2211.05783v1.Unifying_Flow_Stereo_and_Depth_Estimation.pdf',\n",
       " './corpus/2211.05781v1.Demystify_Transformers_Convolutions_in_Modern_Image_Deep_Networks.pdf',\n",
       " './corpus/2211.05778v1.InternImage_Exploring_Large_Scale_Vision_Foundation_Models_with_Deformable_Convolutions.pdf',\n",
       " './corpus/2211.05776v1.Fine_Grained_Entity_Segmentation.pdf',\n",
       " './corpus/2211.05774v1.NLO_computation_of_diffractive_di_hadron_production_in_a_saturation_framework.pdf',\n",
       " './corpus/2211.05773v1.Scaling_Neural_Face_Synthesis_to_High_FPS_and_Low_Latency_by_Neural_Caching.pdf',\n",
       " './corpus/2211.05770v1.StyleNAT_Giving_Each_Head_a_New_Perspective.pdf',\n",
       " './corpus/2211.05762v1.Efficient_brain_age_prediction_from_3D_MRI_volumes_using_2D_projections.pdf',\n",
       " './corpus/2211.05758v1.Cloaking_a_qubit_in_a_cavity.pdf',\n",
       " './corpus/2211.05756v1.Massively_Multilingual_ASR_on_70_Languages_Tokenization_Architecture_and_Generalization_Capabilities.pdf',\n",
       " './corpus/2211.05750v1.Nano_Nested_Human_in_the_Loop_Reward_Learning_for_Few_shot_Language_Model_Control.pdf',\n",
       " './corpus/2211.05749v1.Sketched_Gaussian_Model_Linear_Discriminant_Analysis_via_the_Randomized_Kaczmarz_Method.pdf',\n",
       " './corpus/2211.05739v1.FedLesScan_Mitigating_Stragglers_in_Serverless_Federated_Learning.pdf',\n",
       " './corpus/2211.05737v1.Canonical_Subproblems_for_Robot_Inverse_Kinematics.pdf',\n",
       " './corpus/2211.05733v1.RAPIDx_High_performance_ReRAM_Processing_in_Memory_Accelerator_for_Sequence_Alignment.pdf',\n",
       " './corpus/2211.05732v1.The_Sample_Complexity_of_Online_Contract_Design.pdf',\n",
       " './corpus/2211.05730v1.NEON_Enabling_Efficient_Support_for_Nonlinear_Operations_in_Resistive_RAM_based_Neural_Network_Accelerators.pdf',\n",
       " './corpus/2211.05728v1.Quantum_Power_Flows_From_Theory_to_Practice.pdf',\n",
       " './corpus/2211.05725v1.Quantum_key_distribution_rates_from_semidefinite_programming.pdf',\n",
       " './corpus/2211.05722v1.Beam_functions_for_N_jettiness_at_N_3_LO_in_perturbative_QCD.pdf']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get <corpus_size> documents and save them to ./corupus folder\n",
    "documents_paths = fetch_documents(corpus_size, corpus_topic, corpus_folder)\n",
    "documents_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erkKWEe8HjqR"
   },
   "source": [
    "### Calculating similarity between two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nDCtNWXyBvWy",
    "outputId": "500ff99d-1725-4d0e-981b-df207f23ddf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./corpus/2211.05776v1.Fine_Grained_Entity_Segmentation.pdf\n",
      "./corpus/2211.05774v1.NLO_computation_of_diffractive_di_hadron_production_in_a_saturation_framework.pdf\n"
     ]
    }
   ],
   "source": [
    "# Choose documents to run similarity\n",
    "document_path_1 = documents_paths[3]\n",
    "document_path_2 = documents_paths[4]\n",
    "print(document_path_1)\n",
    "print(document_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOTPEPfLA8va",
    "outputId": "72f05e59-43cc-4c98-83ba-d380c6cf3131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9008617205131516030 -4677812717532653284 -6256740798792492484\n",
      "  8955424798090466751   526017995009675404 -6793041897300080260\n",
      "  4055162371009629887 -7113504791994472317  1976490998424331438\n",
      "  6543118162725277669 -9145806595992488346 -8234806505739044653\n",
      " -6855271931156039237  3611541733218742969 -1946880242280710990\n",
      "  2711045691284377493 -9141610277465793534  7930181600597119843\n",
      "  6697458469881874190 -2323150647659221207   368941763971029525\n",
      "  -696798172968316397  6135738553978664476  6610254653168631231\n",
      "  1924990847647532312 -4033009840720180231  7464076158254673952\n",
      "  7370746350461274684   496315090523836829 -2143419140190699035\n",
      " -2312024509991848945  -612359370047413659 -5710722816567520663\n",
      "  2334967328176408570   145760359513007942 -7034198718089086513\n",
      "   738225666598516436  1576459389802616464 -2189387434414780493\n",
      " -7533479285185731322 -4611387064690718518 -3319196109805955600\n",
      " -4357991188474503078 -3633319610782276434  -179670136888409325\n",
      "  6739624019729396715 -7683532318750448917  8539474716483190835\n",
      "  3000679385204606188  -174825640565977158  5889640319495052669\n",
      " -7468006292458226485  6716030239228881380  6283450569447867102\n",
      " -4497285928497226244 -6458367212676400641  3831145109038138625\n",
      " -3297805617980577312  7335399304751572545 -4022769061716562686\n",
      " -6422030710580263750 -2851279030124792003 -4714227880129468611\n",
      " -2918479121602615629  1058510622804276515  3351309751644376488\n",
      "  7522916954761551002  1765899679806795309 -8286642967292506674\n",
      "  5519514802209022374  6145830589531806704 -5457347177662125935\n",
      "  8540371493402259463  7965981054706517545  8955650747959735057\n",
      " -1485583231726991409  5837930576104833837 -5854438954635000043\n",
      "  3842769270909787336 -6796213058404458608  3023728414764496380\n",
      " -4900777177218625033 -4163138874607875454  8610911406174424404\n",
      " -5747346674485665711 -5175158565131885759  2492168273602419223\n",
      " -3443365452526720693 -4300914053516431146 -5160072124228170006\n",
      "  1164429728482174042  6270167177920694964  5402479442708679724\n",
      " -4194893481323124029  4378789544455916425  6400370473227939872\n",
      "  -277340364734148364 -5528077775762619614 -6167742849869449125\n",
      " -7883725270739789041]\n",
      "[ 4613695088196626053  2383545427941341176 -5749017019273748287\n",
      " -7094744584517764018  8604485757896396836  4428212571741509684\n",
      "  2409131476469303616 -7575043070536399693  8336967691757498554\n",
      " -2398772531453136822 -7723455908493053664  1985475480400049772\n",
      "  2790158671982543881  8863883739756610884 -8250745225344449285\n",
      "  3781269085247861233 -2811816194649430925 -8928145845535570562\n",
      " -1671207852258349210  6541995585696081383 -2055159613918076297\n",
      "  8266306547124237119  1490374043087643296 -6939765920411526261\n",
      " -4193964036038112221  3950348788773636164 -8929213754340672467\n",
      "  6280401569748830003   472569712550353209 -8652402121327859667\n",
      " -2917630497386782652   325101035503745447 -3211549549502191806\n",
      " -5548871799477415859  6000548363255136233 -3084061379129736555\n",
      "  3228867588088620503  2214926748140452407 -7434671472961298015\n",
      "  8509412724015771624 -5058606877520379697 -1776034691450085334\n",
      "  5165320706022709997 -2207745170331807515  3373810368435071111\n",
      "    48454695171366930  8139430324835517824   126466694271201609\n",
      "  8947244497166386649  5924476481623478822 -7281631072497851930\n",
      " -6757760416803204450 -2857101773649792320 -8137124430593622710\n",
      "  7287821644232074893 -5201089046792026875 -5500603421538641698\n",
      "  8780736726430871437  5856773476728509084  8217117765762176961\n",
      " -1184345265694263525 -8227900549209288841 -2803908969774154384\n",
      " -6520352451622680706   853931397442997922  3812670765226417308\n",
      "  1779543362597323147 -8756764267755314559  2439914340952895331\n",
      " -4696949197553227557   928135415382935311 -2349309712959743331\n",
      "  3244187952191280253 -8779023580354975537 -1926588982559389444\n",
      "  7012724105846797726 -2707790545048097498  4521887964159679260\n",
      "  1795212272020898532  6311346949077745721  3232149759516444336\n",
      "  8463943147788299571  -710593844163961361 -5440910380734123918\n",
      "  -739363285470612250 -1210778078879914445 -1155321284540968231\n",
      "  2997314269744767271  6709405726837184585  4767930854960740462\n",
      " -7975676031316692949  6755925029196967457  9037295424344836889\n",
      "  5253629410840641798  4114632052360781365 -6469513948773366312\n",
      " -5285950419172609698 -7414951484790238705  7558799600108589151\n",
      "  3117254058674372759]\n"
     ]
    }
   ],
   "source": [
    "# Get set of hashed shingles of first two documents\n",
    "shingles_1 = get_shingles_from_document(document_path_1, shingle_size)\n",
    "shingles_2 = get_shingles_from_document(document_path_2, shingle_size)\n",
    "print(shingles_1[:100])\n",
    "print(shingles_2[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fZ9-oTaHCV2",
    "outputId": "ab6fdb66-73bd-424f-c4df-e92769fe3c9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012098762841163076"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get jaccard similarity from hashed shingles\n",
    "get_jaccard_similarity_from_shingles(shingles_1, shingles_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "FPPhkUgFlizg"
   },
   "outputs": [],
   "source": [
    "# For every document to have same hash functions\n",
    "a_minhash_coefficients = random_coefficients(signature_length) # lists of randomly generated coefficients that will be used for minhashing\n",
    "b_minhash_coefficients = random_coefficients(signature_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LbAjTGs0r5ZC",
    "outputId": "835b3fe6-4d21-4674-9622-930fe2c01f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 70159  56689 183098  16303  34885  29845  47578   1997   6085  27033\n",
      "   1033     41  36940 175463  34115  10601  58679  89139 191586 151373]\n"
     ]
    }
   ],
   "source": [
    "# Get minhash signatures of hashed shingles\n",
    "signature_1 = minhashing(signature_length, shingles_1, a_minhash_coefficients, b_minhash_coefficients, max_hash_minhash)\n",
    "signature_2 = minhashing(signature_length, shingles_2, b_minhash_coefficients, b_minhash_coefficients, max_hash_minhash)\n",
    "print(signature_2[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYu3RXivtCw-",
    "outputId": "702a32b5-8063-43dc-8899-31f73e44d4cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get similarity of signatures\n",
    "get_similarity_minhash_signatures(signature_1, signature_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZByccFf6twJA",
    "outputId": "74c10202-5366-4690-b339-97bc6482b454"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSH\n",
    "lsh([signature_1, signature_2], signature_length, 0.03, lsh_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zssWvs9avVyQ",
    "outputId": "9b662004-42a0-4d7e-8f8f-4753a3e6d923"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSH\n",
    "lsh([signature_1, signature_2], signature_length, 0.05, lsh_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUXpHmjRy6GL"
   },
   "source": [
    "### Calculate similarity for all documents inside the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "-cLsi0Ciy9l7"
   },
   "outputs": [],
   "source": [
    "corpus_shingles = []\n",
    "start = time.time()\n",
    "for document_path_index in range(len(documents_paths)): \n",
    "  document_path = documents_paths[document_path_index]\n",
    "  shingles = get_shingles_from_document(document_path, shingle_size)\n",
    "\n",
    "  # Important: Increment similarity between signatures by adding the previous shingles\n",
    "  # Just for forcing to be similar (it can be commented for real similarity, but in real papers it would probably not output anything)\n",
    "  if document_path_index != 0:\n",
    "    shingles = np.append(shingles, corpus_shingles[document_path_index - 1])\n",
    "    \n",
    "  corpus_shingles.append(shingles)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dU3__XX7B2IH",
    "outputId": "99b1e315-2bc2-4f15-9ae4-46ddeaf0e3ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of the generating shingles was 57.90902400016785\n",
      "Average per document: 2.8954512000083925\n"
     ]
    }
   ],
   "source": [
    "print(f\"Runtime of the generating shingles was {end - start}\")\n",
    "print(f\"Average per document: {(end - start) / len(documents_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "evmIXTtp9u3E"
   },
   "outputs": [],
   "source": [
    "## Filter shingles with less than million rows\n",
    "corpus_shingles = list(filter(lambda corpus_shingle: len(corpus_shingle) > 100000, corpus_shingles))\n",
    "\n",
    "## Trim shingles to max 200000, so they are between 100k and 200k\n",
    "corpus_shingles = [corpus_shingle[:150000] for corpus_shingle in corpus_shingles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "Samatrj1ljuT"
   },
   "outputs": [],
   "source": [
    "# For every document to have same hash functions\n",
    "a_minhash_coefficients = random_coefficients(signature_length) # lists of randomly generated coefficients that will be used for minhashing\n",
    "b_minhash_coefficients = random_coefficients(signature_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IdwgNA4nBFdP",
    "outputId": "390e82de-2e75-44ab-fd9d-f935fbaabace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of the generating signatures was 7.586824893951416\n",
      "Average per document: 0.3793412446975708\n"
     ]
    }
   ],
   "source": [
    "corpus_signatures = []\n",
    "start = time.time()\n",
    "\n",
    "for shingle in corpus_shingles:\n",
    "  signature = minhashing(signature_length, shingle, a_minhash_coefficients, b_minhash_coefficients, max_hash_minhash)\n",
    "  corpus_signatures.append(signature)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Runtime of the generating signatures was {end - start}\")\n",
    "print(f\"Average per document: {(end - start) / len(documents_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Mgcji-mzsSt",
    "outputId": "329b7889-6db3-4767-e297-38840822e471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of LSH was 0.05121612548828125\n",
      "Average per document: 0.0025608062744140623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(4, 5), (6, 7)}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSH\n",
    "start = time.time()\n",
    "candidate_pairs = lsh(corpus_signatures, signature_length, 0.80, max_hash_minhash)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Runtime of LSH was {end - start}\")\n",
    "print(f\"Average per document: {(end - start) / len(documents_paths)}\")\n",
    "candidate_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RM9oVuWH8jzF",
    "outputId": "e1cdea66-2027-4bf0-b7b4-ead042a401c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Candidate pair: (6, 7)\n",
      "\t Jaccard Similarity: 0.702185606318513\n",
      "\t Minhash Signature Similarity: 0.782\n",
      "\n",
      "\n",
      "\t Candidate pair: (4, 5)\n",
      "\t Jaccard Similarity: 0.7730182088970053\n",
      "\t Minhash Signature Similarity: 0.838\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for candidate_pair in candidate_pairs:\n",
    "  i = candidate_pair[0]\n",
    "  j = candidate_pair[1]\n",
    "\n",
    "  print(\"\\t Candidate pair: \" + str(candidate_pair))\n",
    "  \n",
    "  # Same method than on calculating signature\n",
    "  jaccard_similarity = get_jaccard_similarity_from_shingles(corpus_shingles[i], corpus_shingles[j])\n",
    "\n",
    "  print(\"\\t Jaccard Similarity: \" + str(jaccard_similarity))\n",
    "  minhash_similarity = get_similarity_minhash_signatures(corpus_signatures[i], corpus_signatures[j])\n",
    "\n",
    "  print(\"\\t Minhash Signature Similarity: \" + str(minhash_similarity))\n",
    "  print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "finding-similar-items.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
